{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Commercial Aviation Fatalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/reducing-commercial-aviation-fatalities/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal,stats,interpolate\n",
    "from biosppy.signals import ecg,resp,eda,eeg\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import log_loss,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to reduce memory usage of a pandas dataframe by checking the range of values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df): \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if (col_type != object):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ecg_r_features(pilot_raw_data):\n",
    "    # Function to generate heart rate and respiration rate features from 'ecg' and 'r' signals\n",
    "    pilot = pilot_raw_data.iloc[0]['pilot']\n",
    "    N = len(pilot_raw_data)\n",
    "    FS = 256 # Sampling frequency\n",
    "    new_pilot_data = pd.DataFrame()\n",
    "    if 'id' in pilot_raw_data.columns:\n",
    "        new_pilot_data['id'] = pilot_raw_data['id']\n",
    "    else:\n",
    "        new_pilot_data['id'] = [str(pilot) + \"_\" + str(i) for i in pilot_raw_data.index.values]\n",
    "        \n",
    "    new_pilot_data['crew'] = pilot_raw_data['crew']\n",
    "    new_pilot_data['seat'] = pilot_raw_data['seat']\n",
    "    new_pilot_data['time'] = pilot_raw_data['time']\n",
    "    new_pilot_data['ecg'] = pilot_raw_data['ecg']\n",
    "    new_pilot_data['r'] = pilot_raw_data['r']\n",
    "    # It was found that some pilots have ecg value = 0, maybe due to some problem in measuring ecg values\n",
    "    # We are imputing new feature values for such pilots with median values for those features\n",
    "    if all(v==0 for v in pilot_raw_data['ecg'].values):\n",
    "        new_pilot_data['heart_rate'] = [62.730858] * N\n",
    "        new_pilot_data['heart_rate_diff'] = [-1.8538252e-05] * N\n",
    "        new_pilot_data['resp_rate'] = [0.25485292] * N\n",
    "        new_pilot_data['resp_rate_diff'] = [-2.862812e-07] * N\n",
    "    else:\n",
    "        # https://biosppy.readthedocs.io/en/stable/biosppy.signals.html#biosppy-signals-ecg\n",
    "        ts,filtered,rpeaks,templates_ts,templates,heart_rate_ts,heart_rate = ecg.ecg(signal=pilot_raw_data['ecg'].values, sampling_rate=FS, show=False)\n",
    "        #The heart rate will be given as samples at different points from the above line. We can extrapolate these samples to fit the whole data\n",
    "        f = interpolate.interp1d(heart_rate_ts, heart_rate, kind='cubic', fill_value='extrapolate')\n",
    "        new_pilot_data['heart_rate'] = f(pilot_raw_data['time'].values)\n",
    "        new_pilot_data['heart_rate_diff'] = new_pilot_data['heart_rate'].diff().fillna(0)\n",
    "        \n",
    "        #https://biosppy.readthedocs.io/en/stable/biosppy.signals.html#biosppy-signals-resp\n",
    "        ts, filtered, zeros, resp_rate_ts, resp_rate_samples = resp.resp(signal=pilot_raw_data['r'].values, sampling_rate=FS, show=False)\n",
    "        #The respiration rate will be given as samples at different points from the above line. We can inpolate these samples to fit the whole data\n",
    "        f = interpolate.interp1d(resp_rate_ts, resp_rate_samples, kind='cubic', fill_value='extrapolate')\n",
    "        new_pilot_data['resp_rate'] = f(pilot_raw_data['time'].values)\n",
    "        new_pilot_data['resp_rate_diff'] = new_pilot_data['resp_rate'].diff().fillna(0)\n",
    "    return new_pilot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gsr_features(pilot_raw_data):\n",
    "    # Function to generate amplitude, peaks and other features from 'gsr' signal\n",
    "    pilot = pilot_raw_data.iloc[0]['pilot']\n",
    "    N = len(pilot_raw_data)\n",
    "    FS = 256 # Sampling frequency\n",
    "    new_pilot_data = pd.DataFrame()\n",
    "    if 'id' in pilot_raw_data.columns:\n",
    "        new_pilot_data['id'] = pilot_raw_data['id']\n",
    "    else:\n",
    "        new_pilot_data['id'] = [str(pilot) + \"_\" + str(i) for i in pilot_raw_data.index.values]\n",
    "    \n",
    "    new_pilot_data['gsr'] = pilot_raw_data['gsr']\n",
    "    # It was found that some pilots have gsr value = 0 for the entire duration, maybe due to some problem in measuring gsr values\n",
    "    # We are imputing new feature values for such pilots with zero for those features\n",
    "    if all(v==0 for v in pilot_raw_data['gsr'].values):\n",
    "        new_pilot_data['gsr_diff'] = [0] * N\n",
    "        new_pilot_data['gsr_diff_2'] = [0] * N\n",
    "        new_pilot_data['gsr_last_onset'] = [N/FS] * N\n",
    "        new_pilot_data['gsr_last_peak'] = [N/FS] * N\n",
    "        new_pilot_data['gsr_amplitude'] = [0] * N\n",
    "    else:\n",
    "        new_pilot_data['gsr_diff'] = new_pilot_data['gsr'].diff().fillna(0)\n",
    "        new_pilot_data['gsr_diff_2'] = new_pilot_data['gsr_diff'].diff().fillna(0)\n",
    "        ts,filtered,onsets,peaks,amplitudes = eda.eda(signal=pilot_raw_data['gsr'].values, sampling_rate=FS, show=False)\n",
    "        last_onset = [N] * N\n",
    "        if len(onsets) > 0:\n",
    "            for i in range(1,len(onsets)):\n",
    "                last_onset[onsets[i-1]:onsets[i]] = list(range(0,(onsets[i]-onsets[i-1])))\n",
    "            last_onset[onsets[-1]:] = list(range(0,(N-onsets[-1])))\n",
    "        last_onset = [i/FS for i in last_onset]\n",
    "        new_pilot_data['gsr_last_onset'] = last_onset\n",
    "        \n",
    "        last_peak = [N] * N\n",
    "        if len(peaks) > 0:\n",
    "            for i in range(1,len(peaks)):\n",
    "                last_peak[peaks[i-1]:peaks[i]] = list(range(0,(peaks[i]-peaks[i-1])))\n",
    "            last_peak[peaks[-1]:] = list(range(0,(N-peaks[-1])))\n",
    "        last_peak = [i/FS for i in last_peak]\n",
    "        new_pilot_data['gsr_last_peak'] = last_peak\n",
    "        \n",
    "        if len(onsets) < 4:\n",
    "            new_pilot_data['gsr_amplitude'] = [0] * N\n",
    "        else:\n",
    "            f_amp = interpolate.interp1d(onsets, amplitudes, kind='cubic', fill_value='extrapolate')\n",
    "            new_pilot_data['gsr_amplitude'] = f_amp(pilot_raw_data['time'].values)\n",
    "    return new_pilot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eeg_features(pilot_raw_data):\n",
    "    # Function to generate power features from 'eeg' signals\n",
    "    pilot = pilot_raw_data.iloc[0]['pilot']\n",
    "    N = len(pilot_raw_data)\n",
    "    FS = 256 # Sampling frequency\n",
    "    EEG_SIGNAL_NAMES = ['eeg_fp1', 'eeg_f7', 'eeg_f8', 'eeg_t4', 'eeg_t6', 'eeg_t5', 'eeg_t3', 'eeg_fp2', \n",
    "                    'eeg_o1', 'eeg_p3', 'eeg_pz', 'eeg_f3', 'eeg_fz', 'eeg_f4', 'eeg_c4', 'eeg_p4', \n",
    "                    'eeg_poz', 'eeg_c3', 'eeg_cz', 'eeg_o2']\n",
    "    new_pilot_data = pd.DataFrame()\n",
    "    if 'id' in pilot_raw_data.columns:\n",
    "        new_pilot_data['id'] = pilot_raw_data['id']\n",
    "    else:\n",
    "        new_pilot_data['id'] = [str(pilot) + \"_\" + str(i) for i in pilot_raw_data.index.values]\n",
    "    \n",
    "    eeg_signals = pilot_raw_data[EEG_SIGNAL_NAMES].values\n",
    "    # https://biosppy.readthedocs.io/en/stable/biosppy.signals.html#biosppy-signals-eeg\n",
    "    ts,theta,alpha_low,alpha_high,beta,gamma = eeg.get_power_features(signal=eeg_signals, sampling_rate=FS, size=40, overlap=0.99375)\n",
    "    for i in range(len(EEG_SIGNAL_NAMES)):\n",
    "        f_theta = interpolate.interp1d(ts, theta[:,i], kind='cubic', fill_value='extrapolate')\n",
    "        new_pilot_data[EEG_SIGNAL_NAMES[i]+\"_theta\"] = f_theta(pilot_raw_data['time'].values)\n",
    "        f_alpha_low = interpolate.interp1d(ts, alpha_low[:,i], kind='cubic', fill_value='extrapolate')\n",
    "        new_pilot_data[EEG_SIGNAL_NAMES[i]+\"_alpha_low\"] = f_alpha_low(pilot_raw_data['time'].values)\n",
    "        f_alpha_high = interpolate.interp1d(ts, alpha_high[:,i], kind='cubic', fill_value='extrapolate')\n",
    "        new_pilot_data[EEG_SIGNAL_NAMES[i]+\"_alpha_high\"] = f_alpha_high(pilot_raw_data['time'].values)\n",
    "        f_beta = interpolate.interp1d(ts, beta[:,i], kind='cubic', fill_value='extrapolate')\n",
    "        new_pilot_data[EEG_SIGNAL_NAMES[i]+\"_beta\"] = f_beta(pilot_raw_data['time'].values)\n",
    "        f_gamma = interpolate.interp1d(ts, gamma[:,i], kind='cubic', fill_value='extrapolate')\n",
    "        new_pilot_data[EEG_SIGNAL_NAMES[i]+\"_gamma\"] = f_gamma(pilot_raw_data['time'].values)\n",
    "\n",
    "    if 'event' in pilot_raw_data.columns:\n",
    "        new_pilot_data['event'] = pilot_raw_data['event']\n",
    "    return new_pilot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_features(pilot_raw_data):\n",
    "    # Function to combine all the new features\n",
    "    ecg_r_features = get_ecg_r_features(pilot_raw_data)\n",
    "    gsr_features = get_gsr_features(pilot_raw_data)\n",
    "    eeg_features = get_eeg_features(pilot_raw_data)\n",
    "    \n",
    "    pilot_new_features = pd.merge(ecg_r_features, gsr_features, on='id')\n",
    "    pilot_new_features = pd.merge(pilot_new_features, eeg_features, on='id')\n",
    "    pilot_new_features = reduce_mem_usage(pilot_new_features)\n",
    "    return pilot_new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return final predictions on the given raw input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(X):\n",
    "    # Preprocessing\n",
    "    exp_mapping = {'CA':0, 'DA':1, 'SS':2, 'LOFT':3}\n",
    "    X['experiment'] = X['experiment'].map(exp_mapping)\n",
    "    X['pilot'] = X['crew'] * 100 + X['experiment'] * 10 + X['seat']\n",
    "    pilots = [pilot for pilot in X['pilot'].unique()]\n",
    "    \n",
    "    # Feature engineering and prediction\n",
    "    predictions = []\n",
    "    model = pickle.load(open('rf_model.sav', 'rb')) # Random Forest model which has been selected for final predictions\n",
    "    crew_ohe = pickle.load(open('crew_ohe.sav','rb')) # One hot encoder for crew id, which has already been trained\n",
    "    crew_ohe_features = [\"crew_\"+str(i) for i in crew_ohe.categories_[0]]\n",
    "    for pilot in tqdm(pilots):\n",
    "        # Check if new features are already generated and stored in a file, else generate them\n",
    "        if os.path.isfile(\"new_features/\"+str(pilot)+\".gzip\"):\n",
    "            pilot_new_features = pd.read_parquet(\"new_features/\"+str(pilot)+\".gzip\")\n",
    "        else:\n",
    "            pilot_raw_data = X.loc[X['pilot'] == pilot]\n",
    "            pilot_raw_data = pilot_raw_data.iloc[pilot_raw_data['time'].argsort()] # Sorting data according to time for each pilot\n",
    "            pilot_raw_data.reset_index(drop=True, inplace=True)\n",
    "            pilot_new_features = get_new_features(pilot_raw_data)\n",
    "            pilot_new_features[crew_ohe_features] = crew_ohe.transform(pilot_new_features[['crew']]).toarray()\n",
    "            for feature in crew_ohe_features:\n",
    "                pilot_new_features[feature] = pilot_new_features[feature].astype(np.int8)\n",
    "            pilot_new_features.drop(['crew'], axis=1, inplace=True)\n",
    "        \n",
    "        df_pred = pd.DataFrame()\n",
    "        df_pred['id'] = pilot_new_features['id']\n",
    "        pilot_new_features = pilot_new_features.drop(['id'], axis=1)\n",
    "        y_pred = model.predict_proba(pilot_new_features)\n",
    "        df_pred['A'] = y_pred[:,0]\n",
    "        df_pred['B'] = y_pred[:,3]\n",
    "        df_pred['C'] = y_pred[:,1]\n",
    "        df_pred['D'] = y_pred[:,2]\n",
    "        predictions.append(df_pred)\n",
    "    predictions = pd.concat(predictions, ignore_index=True)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [09:55<00:00, 33.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 585M/585M [00:07<00:00, 85.5MB/s]\n",
      "Successfully submitted to Reducing Commercial Aviation Fatalities"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_csv('test.csv')\n",
    "predictions = get_predictions(X_test)\n",
    "predictions.to_csv(\"final_submission.csv\", index=False)\n",
    "!kaggle competitions submit -c reducing-commercial-aviation-fatalities -f final_submission.csv -m \"This output was generated by Random Forest model which was selected for final submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/NKif5oi.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return log loss score on given raw input data and target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_score(X,y):\n",
    "    X['event'] = y\n",
    "    # Preprocessing\n",
    "    event_mapping = {'A': 0, 'C': 1, 'D': 2, 'B': 3}\n",
    "    X['event'] = X['event'].map(event_mapping)\n",
    "    exp_mapping = {'CA':0, 'DA':1, 'SS':2, 'LOFT':3}\n",
    "    X['experiment'] = X['experiment'].map(exp_mapping)\n",
    "    X['pilot'] = X['crew'] * 100 + X['experiment'] * 10 + X['seat']\n",
    "    pilots = [pilot for pilot in X['pilot'].unique()]\n",
    "    \n",
    "    # Feature engineering and prediction\n",
    "    predictions = []\n",
    "    target_values = []\n",
    "    model = pickle.load(open('rf_model.sav', 'rb')) # Random Forest model which has been selected for final predictions\n",
    "    crew_ohe = pickle.load(open('crew_ohe.sav','rb')) # One hot encoder for crew id, which has already been trained\n",
    "    crew_ohe_features = [\"crew_\"+str(i) for i in crew_ohe.categories_[0]]\n",
    "    for pilot in tqdm(pilots):\n",
    "        # Check if new features are already generated and stored in a file, else generate them\n",
    "        if os.path.isfile(\"new_features/\"+str(pilot)+\".gzip\"):\n",
    "            pilot_new_features = pd.read_parquet(\"new_features/\"+str(pilot)+\".gzip\")\n",
    "        else:\n",
    "            pilot_raw_data = X.loc[X['pilot'] == pilot]\n",
    "            pilot_raw_data = pilot_raw_data.iloc[pilot_raw_data['time'].argsort()] # Sorting data according to time for each pilot\n",
    "            pilot_raw_data.reset_index(drop=True, inplace=True)\n",
    "            pilot_new_features = get_new_features(pilot_raw_data)\n",
    "            pilot_new_features[crew_ohe_features] = crew_ohe.transform(pilot_new_features[['crew']]).toarray()\n",
    "            for feature in crew_ohe_features:\n",
    "                pilot_new_features[feature] = pilot_new_features[feature].astype(np.int8)\n",
    "            pilot_new_features.drop(['crew'], axis=1, inplace=True)\n",
    "            pilot_new_features.to_parquet(\"new_features/\"+str(pilot)+\".gzip\",compression='gzip')\n",
    "            \n",
    "        y = pilot_new_features['event']\n",
    "        pilot_new_features = pilot_new_features.drop(['id','event'], axis=1)\n",
    "        y_pred = model.predict_proba(pilot_new_features)\n",
    "        predictions.extend(y_pred)\n",
    "        target_values.extend(y)\n",
    "    loss = log_loss(target_values, predictions)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [04:19<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi class log loss for the given input and target values is  0.00035128547908662216\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "y = df_train['event']\n",
    "X = df_train.drop(['event'], axis=1)\n",
    "loss = get_score(X,y)\n",
    "print('Multi class log loss for the given input and target values is ',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
